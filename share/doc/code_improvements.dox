/** @page code_improvements Points to improve the code

@todo is this page uptodate?

@section ci_penetration Penetrating collisions
The way penetrating collisions are
handled is really not nice ors_swift.cpp:318. What should ideally
happen: find the two points on the meshes that penetrate most, set $d$
as their negative distance, and the penetration normal aligned with
their distance. What happens now: set the distance $d=0$ and set the
normal align with the shape centers. The problem: the collision
gradient is not really correct.

What to do: Actually most other collision libs focus on penetration
(bullet/ode/solid), only SWIFT focusses on distance queries. What one
would have to do, call one of these other collision libs to compute
the most-penetrating two points of convex shapes.



@subsection ci_marc Marc's older notes for code


- Processes may not access MT.cfg in their step routine (open() is
  acceptable, but still not optimal).
  Instead, if necessary, parameters need to be made a proper
  variable. A special gui process offers a way to modify the variable
  online by the user.
- The Lock (from process_internal.h) should actually not be used. The
  fact that it is used right now shows bad programming: It is always
  used when one process (or main loop) accesses directly another
  process instead of via a variable. Change that!
- Make some central |ors| variable, or so, from which
  processes can update their ors-structure. Perhaps that needs
  some signalling -- currently this orsCopyStuff after the
  connectivity is changed (object glued to hand/released) is not at all clean
  (related to point above).
- The motion planners so far only optimize the next to go
  motion. Ideally there should be parallel planners optimizing already
  future motions \f$\to\f$ Dmitry's keyframe approach.
- Make a Variable out of the TaskVariableList!!
- All variables should have setters and getters of the format
  arr get_qreal(Process*); void set_qreal(Process*,const arr&);
  These throw errors if called from a process that hasn't locked the
  variable or is a 'set' is called in readLock.
- robot.cpp:281: read the current state in the schunk.open() routine!  not here!
- remove setting accelerations in each step of schunk hand motion
  - use radians (SDH constructor) out of the box
  - set and get velocites with one function call to schunk lib
- SOC: make consistent to what is written in the TOMSY document
- GRASP HEURISTICS: 1) Instead of conditioning velocities for lifting
  and placing, condition positions directly. 2) For grasp-approach:
  instead of conditioning only final position, condition on an
  approach position-profile: namely in the reference frame
  object-relative-to-hand. In both cases, use sinus position profiles?


@subsection ci_nikolay_vision Nikolay's ideas for vision
- improve quality of color segmentation, while staying cheap
  computationally: learn a discriminative classifier \f$f(x)\f$ to
  separate by HSV color pixel of interest \f$x\f$ from background
- to get an intuitive interface without too much program overhead:
  create mask of desired discrimination as an image and train
  classifier in external program (matlab or C++), to output model
  parameters
- use a mixture of \f$k\f$ Gaussians for the activation of pixel \f$x\f$
  in HSV space: \f$f(x) = \sum_k \alpha_k exp(- \frac{\|c_k -
    x\|^2}{\sigma_k})\f$ , probably 2 Gaussians are already more than
  enough, the current code is equivalent to \f$k=1\f$ single Gaussian
- just load parameters of trained Gaussian mixture and incorporate
  in earlyvisionmodule.step(): for each gaussian \f$k\f$ we will calculate
  one response image and than sum them, to use the existing data
  structures make \f$\sum_k \alpha_k = 1\f$
- optionally, use RGB space in addition to HSV space
- use the above techniques for the LEDtracker... should simplify
  dealing with reflections
- For an example how an image interface can be useful to define
  labels for training discriminative segmentation: few brush strokes
  are enough to define regions of foreground and background see
  frogsegment.jpg, (algorithm implemented by me in matlab)
\end{enumerate}


*/
