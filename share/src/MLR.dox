/**
@mainpage MLR - Machine Learning and Robotics Lab

The MLR project consists of lots of little libs that you can use and build your own program with simple make files.

@note Feel free to add documentation!

The documentation of the libs can be found under "Modules".
Documentation which is not directly connected to the source code (like coding conventions, howtos, tutorials, etc.) is under "Related Pages"

@note If you're new to the project start reading the @ref page_getting_started and the @ref coding_conventions.



@section overview Overview
Scattered throughout the project there are a few READMEs. Read them!

Here is a overview of the mlr project structure:
@verbatim
.
├── install                 # install scipts to install mlr
├── projects
├── share                   # all the stuff that can be used by everybody
│   ├── bin                 # helpful scripts
│   ├── configurations
│   ├── data
│   ├── demo
│   ├── doc                 # doxygen docu and more
│   ├── extern              # external libs live here
│   ├── include
│   ├── lib                 # the libs we 'make' live here
│   ├── src                 # the acutal source code:
│   │   │                   # - each user has a folder for his projects
│   │   │                   # - useful projects are extracted to their own folders
│   │   │                   # - each folder represents a lib that you can use
│   │   ├── MT
│   │   ├── <USER_X>
│   │   ├── ...
│   │   ├── biros
│   │   ├── devTools
│   │   ├── hardware
│   │   ├── motion
│   │   ├── perception
│   │   ├── relational
│   │   └── views
│   └── test                # test (not unittest) and example go here
├── slices                  # extracted libs that are published seperately
├── tools
└── usr                     # try out your stuff in your folder.
    ├── MT
    ├── ...
    └── <USER_X>
@endverbatim

@subsection develop_code How to develop you code
-# develop you code isolated in usr/USER/
-# if it's useful put it under share/USER/. Everybody can use it but USER is still responsible for it.
-# move packages that are useful and a coherent unit to share/PACKAGE. Everybody owns it.



@section doxygen Documentation

We use Doxygen for our documentation (API and everything else).
- Modules are defined and documented in @a *.dox files.
  - A @a *.dox should contain a high level overview of the module.
  - Everything that belongs to a module should be added to the group.
  - The API documentation is then generated automatically.
- Related Pages contain 'HOWTOs' and general tutorials.
  - they are stored under share/doc/*.dox
- Use existing modules/pages/etc as blueprint and have a look at:
  - Doxygen Homepage: http://www.stack.nl/~dimitri/doxygen/
  - Formulas in Doxygen: http://www.stack.nl/~dimitri/doxygen/manual/formulas.html



@section jenkins Continuous Integration with Jenkins
We're trying to make it easy to install the code and keep a it solid.
Therefore we use a continuous integration server Jenkins which checks out the repository,
builds it (with different settings), executes some tests,
collects some statistics and writes angry emails when your commits broke the build.

Here is the web interface:
http://sully.informatik.uni-stuttgart.de:8080/

If you want to you can add your own tests as well!

More on Jenkins: http://jenkins-ci.org/



@section misc Misc Guides

@subsection misc_coding coding utilities
mlr offers quite a few useful features:
- STRING class
- Parameter read from cmd line or config file
- Plotting in opengl & gnuplot
- opengl using freeglut, fltk or qt



@subsection misc_hardware Guide to the robot hardware (Marc)
This is for the Schunk arm in Berlin.

- Startup hardware
  - turn on robot
  - mlrMountHardware
  - mlrJoystick
  - mlrJoystick -openArm 1 -openHand 1 -openSkin 1
- Debugging hardware:
  - 09-testSchunkBasics
  - 09-testHandMotion
  - 09-testHandSense
- Testing perception:
  - 10-testEarlyVision
  - 10-testPerception
- Learning about the control architecture:
  - 10-miniExample (launches minimal set of Processes explicitly by hand)
  - RobotActionInterface (launches Processes using the RobotActionInterface)
  - 10-planningDemo


@subsection misc_perception Guide to the visual perception software
@todo maybe move this to a doxygen group

The current perception module does a very simple thing:
- We have a left and right image. 'EarlyVision' is computing the
  HSV for these images. We assume to know a specific target \f$hsv^*\in[0,255]^3\f$
  values together standard deviations \f$\sigma_{hsv}\in[0,255]^3\f$. We compute
  the evidence \f$\theta_i = \exp(- (hsv_i-hsv^*)^2/\sigma_{hsv}^2)\f$ [sorry for
    sloppy notation] for each pixel \f$i\f$ in the left and right image.
- Given \f$\theta_i\f$ in an image, we call OpenCV's flood-fill that finds
  the contour of the highest \f$\theta\f$-value region. Let's call the contour
  \f$\partial C\f$. (We do this for both images.)
- Given the contour \f$\partial C\f$ we compute a distance-to-contour
  field/image: for each pixel \f$i\f$ we compute \f$d_i = \min_{j\in\partial C}
  \|i-j\|\f$ (using some OpenCV routine). We do this for both images. The
  image \f$d_i\f$ is a good potential cost function to let 2D contour
  models converge to the HSV contour.
- We have three different parametric 2D contour models: 1) for a
  circle (1 parameter), 2) for a polygon with 6 vertices and parallel
  opposing edges (a bit like a hexagon, can fit any 2D-projected 3D
  box), 3) a contour model that corresponds to a 2D projected
  cylinder.
  We fit a contour model to the HSV contour by minimizing the sum of
  \f$d_i\f$ for all points on the contour model. We do this on both
  images. Fitting is done by gradient descent (RPROP). We get
  parameters of the 2D contours with sub-pixel accuracy.
- Given the fitted 2D contours in the right and left image, we
  triangulate them. Giving us a 2D contour mapped into 3D space. From
  there it is trivial to fit a 3D ball, cylinder, or box.

@note It is not by accident that we stay 2D for until the last step:
in our experience it proved more robust to try to fit shapes/contours
in 2D first with as much accuracy as possible before triangulating.



@subsection misc_optimization Optimization Methods (Marc)
There are quite a bit of generic optimization methods implemented --
but not well documented/organized yet. Ask Marc.

- Rprop (best gradient descent method)
- GaussNewton
- CMA
- Some Genetic & Evolutionary Algorithms (e.g., similar to CMA)



@subsection misc_inference Inference and Machine Learning methods (Marc)
We have probabilistic inference code (infer lib) and also basic
Machine Learning methods (as introduced in the ML lecture). Ask Marc.

- Gaussian Processes
- ridge regression, logistic regression, etc
- MDPs, POMDPs



@subsection misc_robotics_stuf Robotics algorithms (Marc)

- RRTs
- Trajectory Prediction
- etc



@subsection misc_relational Relational RL (Tobias)

- Robot Manipulation Simulator: http://userpage.fu-berlin.de/tlang/RMSim/
- libPRADA, a library for relational planning and rule learning: http://userpage.fu-berlin.de/tlang/prada/




*/
// vim: noai:ts=2:sw=2:set expandtab:
